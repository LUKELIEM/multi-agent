{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-agent Gathering Environment\n",
    "\n",
    "In this notebook, we will get acquainted with the multi-agent Gathering game environment created by HumanCompatibleAI on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.6.4\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import platform\n",
    "\n",
    "from env import GatheringEnv   # This is the Game Environment\n",
    "from model import Policy   # Use the Policy defined below instead\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Python version: \", platform.python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Agent\n",
    "\n",
    "The agent's policy is a simple 2-layer NN. The current code only allows one agent to be trained in a Gathering environment with the other agents being random agents.\n",
    "\n",
    "Note that the weights are saved and loaded from a fixed single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \n",
    "    # an index parameter may be needed to allow for multiple learning agents\n",
    "    def __init__(self, observation_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(observation_size, 128)\n",
    "        self.affine2 = nn.Linear(128, 8)\n",
    "\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores)\n",
    "\n",
    "    # The weights should be allowed to be saved into and loaded different model files\n",
    "    def save_weights(self):\n",
    "        torch.save(self.state_dict(), 'model.pkl')   \n",
    "\n",
    "    def load_weights(self):\n",
    "        self.load_state_dict(torch.load('model.pkl'))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        probs = self(Variable(state))\n",
    "        action = probs.multinomial()\n",
    "        self.saved_actions.append(action)\n",
    "        return action.data[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maps\n",
    "\n",
    "GatheringEnv will render a Gathering map based on text files. The following are some of the available text files:\n",
    "\n",
    "* default.txt  \n",
    "* open.txt  \n",
    "* region_multi_entrance.txt  \n",
    "* region_no_walls.txt  \n",
    "* region_single_entrance.txt  \n",
    "* region_unequal_single_entrance.txt  \n",
    "* small.txt\n",
    "\n",
    "The default map is the map utilized by DeepMind's paper.\n",
    "\n",
    "The current code has several limitations:\n",
    "* N_agents must be 2\n",
    "* The 2nd agent can only be random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggressiveness is 0.05\n"
     ]
    }
   ],
   "source": [
    "env = GatheringEnv(n_agents=2, map_name='default')  # Change map here; n must be 2 or under\n",
    "\n",
    "# Env API is similar to that of OpenAI Gym\n",
    "state_n = env.reset()\n",
    "env.render()\n",
    "\n",
    "# Load previously trained policy agent\n",
    "policy = Policy(env.state_size)\n",
    "policy.load_weights()\n",
    "\n",
    "n_tags = 0 # count number of times learning agent tag the random agent\n",
    "n_steps = 300\n",
    "\n",
    "# Render for 1000 steps\n",
    "for _ in range(n_steps):\n",
    "    action = policy.select_action(state_n[0])\n",
    "    state_n, reward_n, done_n, info_n = env.step([\n",
    "        action,                           # Agent 1 is policy AI\n",
    "        random.randrange(0, 8)            # Agent 2 is random\n",
    "    ])\n",
    "    \n",
    "    if action is 6:\n",
    "        n_tags += 1\n",
    "        \n",
    "    if any(done_n):\n",
    "        break\n",
    "    env.render()\n",
    "    time.sleep(1/30)  # Change speed of video rendering\n",
    "\n",
    "env.close()  # Close the rendering window\n",
    "print (\"Aggressiveness is {:.2f}\".format(n_tags/n_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE\n",
    "\n",
    "The single agent is then trained with the basic policy gradient algorithm REINFORCE.\n",
    "\n",
    "We discard the model.pkl and the hyperparameters provided by HumanCompatibleAI. Using lr = 1e-3 after 1000 training episodes, the agent learns how to collect apples and occasionally laser-tag the random agent:\n",
    "\n",
    "SA_ep1000_lr1e-3_default_r390.pkl\n",
    "\n",
    "After 3000 episodes, agent performs marginally better. It still does not know how to tag the random agent consistently.\n",
    "\n",
    "SA_ep3000_lr1e-3_default_r440.pkl\n",
    "\n",
    "Aggressiveness is only 0.03. Meaning the learning agent is tagging only 3 out of every 100 steps, when the random agent is much more aggressively tagging 1 out of every 8 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20\tLast reward:     0\tAverage reward: 0.20\n",
      "Episode 40\tLast reward:     3\tAverage reward: 0.94\n",
      "Episode 60\tLast reward:     0\tAverage reward: 1.17\n",
      "Episode 80\tLast reward:     1\tAverage reward: 1.51\n",
      "Episode 100\tLast reward:     0\tAverage reward: 2.43\n",
      "Episode 120\tLast reward:    35\tAverage reward: 5.17\n",
      "Episode 140\tLast reward:    51\tAverage reward: 10.74\n",
      "Episode 160\tLast reward:    59\tAverage reward: 16.84\n",
      "Episode 180\tLast reward:   108\tAverage reward: 30.03\n",
      "Episode 200\tLast reward:   161\tAverage reward: 38.59\n",
      "Episode 220\tLast reward:   193\tAverage reward: 53.23\n",
      "Episode 240\tLast reward:   181\tAverage reward: 71.10\n",
      "Episode 260\tLast reward:   175\tAverage reward: 88.67\n",
      "Episode 280\tLast reward:   205\tAverage reward: 106.47\n",
      "Episode 300\tLast reward:   196\tAverage reward: 124.59\n",
      "Episode 320\tLast reward:   227\tAverage reward: 138.81\n",
      "Episode 340\tLast reward:   251\tAverage reward: 155.71\n",
      "Episode 360\tLast reward:   240\tAverage reward: 173.52\n",
      "Episode 380\tLast reward:   230\tAverage reward: 187.31\n",
      "Episode 400\tLast reward:   220\tAverage reward: 201.70\n",
      "Episode 420\tLast reward:   285\tAverage reward: 217.21\n",
      "Episode 440\tLast reward:   300\tAverage reward: 227.13\n",
      "Episode 460\tLast reward:   291\tAverage reward: 233.71\n",
      "Episode 480\tLast reward:   270\tAverage reward: 242.63\n",
      "Episode 500\tLast reward:   298\tAverage reward: 247.45\n",
      "Episode 520\tLast reward:   210\tAverage reward: 252.18\n",
      "Episode 540\tLast reward:   201\tAverage reward: 259.65\n",
      "Episode 560\tLast reward:   293\tAverage reward: 269.37\n",
      "Episode 580\tLast reward:   372\tAverage reward: 281.59\n",
      "Episode 600\tLast reward:   397\tAverage reward: 295.35\n",
      "Episode 620\tLast reward:   383\tAverage reward: 303.19\n",
      "Episode 640\tLast reward:   377\tAverage reward: 312.09\n",
      "Episode 660\tLast reward:   411\tAverage reward: 314.93\n",
      "Episode 680\tLast reward:   368\tAverage reward: 318.80\n",
      "Episode 700\tLast reward:   331\tAverage reward: 328.86\n",
      "Episode 720\tLast reward:   311\tAverage reward: 332.90\n",
      "Episode 740\tLast reward:   372\tAverage reward: 340.07\n",
      "Episode 760\tLast reward:   396\tAverage reward: 350.35\n",
      "Episode 780\tLast reward:   347\tAverage reward: 359.27\n",
      "Episode 800\tLast reward:   422\tAverage reward: 367.06\n",
      "Episode 820\tLast reward:   444\tAverage reward: 371.14\n",
      "Episode 840\tLast reward:   329\tAverage reward: 370.70\n",
      "Episode 860\tLast reward:   368\tAverage reward: 375.64\n",
      "Episode 880\tLast reward:   467\tAverage reward: 379.63\n",
      "Episode 900\tLast reward:   476\tAverage reward: 380.39\n",
      "Episode 920\tLast reward:   442\tAverage reward: 380.40\n",
      "Episode 940\tLast reward:   366\tAverage reward: 379.87\n",
      "Episode 960\tLast reward:   400\tAverage reward: 377.48\n",
      "Episode 980\tLast reward:   501\tAverage reward: 382.57\n",
      "Episode 1000\tLast reward:   341\tAverage reward: 381.94\n",
      "Episode 1020\tLast reward:   447\tAverage reward: 382.32\n",
      "Episode 1040\tLast reward:   449\tAverage reward: 388.29\n",
      "Episode 1060\tLast reward:   378\tAverage reward: 392.48\n",
      "Episode 1080\tLast reward:   378\tAverage reward: 394.40\n",
      "Episode 1100\tLast reward:   348\tAverage reward: 394.76\n",
      "Episode 1120\tLast reward:   479\tAverage reward: 396.65\n",
      "Episode 1140\tLast reward:   253\tAverage reward: 397.64\n",
      "Episode 1160\tLast reward:   430\tAverage reward: 395.84\n",
      "Episode 1180\tLast reward:   388\tAverage reward: 395.60\n",
      "Episode 1200\tLast reward:   379\tAverage reward: 395.68\n",
      "Episode 1220\tLast reward:   237\tAverage reward: 392.90\n",
      "Episode 1240\tLast reward:   362\tAverage reward: 390.82\n",
      "Episode 1260\tLast reward:   363\tAverage reward: 394.48\n",
      "Episode 1280\tLast reward:   335\tAverage reward: 399.65\n",
      "Episode 1300\tLast reward:   353\tAverage reward: 400.87\n",
      "Episode 1320\tLast reward:   411\tAverage reward: 406.82\n",
      "Episode 1340\tLast reward:   424\tAverage reward: 408.89\n",
      "Episode 1360\tLast reward:   438\tAverage reward: 407.85\n",
      "Episode 1380\tLast reward:   423\tAverage reward: 408.91\n",
      "Episode 1400\tLast reward:   468\tAverage reward: 412.66\n",
      "Episode 1420\tLast reward:   388\tAverage reward: 414.44\n",
      "Episode 1440\tLast reward:   410\tAverage reward: 410.84\n",
      "Episode 1460\tLast reward:   279\tAverage reward: 411.18\n",
      "Episode 1480\tLast reward:   444\tAverage reward: 416.94\n",
      "Episode 1500\tLast reward:   492\tAverage reward: 416.50\n",
      "Episode 1520\tLast reward:   438\tAverage reward: 418.68\n",
      "Episode 1540\tLast reward:   343\tAverage reward: 412.40\n",
      "Episode 1560\tLast reward:   337\tAverage reward: 405.33\n",
      "Episode 1580\tLast reward:   305\tAverage reward: 398.17\n",
      "Episode 1600\tLast reward:   396\tAverage reward: 386.28\n",
      "Episode 1620\tLast reward:   391\tAverage reward: 381.39\n",
      "Episode 1640\tLast reward:   377\tAverage reward: 379.08\n",
      "Episode 1660\tLast reward:   472\tAverage reward: 381.57\n",
      "Episode 1680\tLast reward:   403\tAverage reward: 387.12\n",
      "Episode 1700\tLast reward:   447\tAverage reward: 394.29\n",
      "Episode 1720\tLast reward:   518\tAverage reward: 401.03\n",
      "Episode 1740\tLast reward:   373\tAverage reward: 405.78\n",
      "Episode 1760\tLast reward:   427\tAverage reward: 406.69\n",
      "Episode 1780\tLast reward:   387\tAverage reward: 402.45\n",
      "Episode 1800\tLast reward:   427\tAverage reward: 407.34\n",
      "Episode 1820\tLast reward:   442\tAverage reward: 410.77\n",
      "Episode 1840\tLast reward:   480\tAverage reward: 414.43\n",
      "Episode 1860\tLast reward:   428\tAverage reward: 416.14\n",
      "Episode 1880\tLast reward:   501\tAverage reward: 421.47\n",
      "Episode 1900\tLast reward:   533\tAverage reward: 426.79\n",
      "Episode 1920\tLast reward:   451\tAverage reward: 427.24\n",
      "Episode 1940\tLast reward:   494\tAverage reward: 429.27\n",
      "Episode 1960\tLast reward:   356\tAverage reward: 430.60\n",
      "Episode 1980\tLast reward:   448\tAverage reward: 430.87\n",
      "Episode 2000\tLast reward:   498\tAverage reward: 431.66\n",
      "Episode 2020\tLast reward:   490\tAverage reward: 425.17\n",
      "Episode 2040\tLast reward:   448\tAverage reward: 421.71\n",
      "Episode 2060\tLast reward:   508\tAverage reward: 427.76\n",
      "Episode 2080\tLast reward:   465\tAverage reward: 433.66\n",
      "Episode 2100\tLast reward:   522\tAverage reward: 425.33\n",
      "Episode 2120\tLast reward:   363\tAverage reward: 420.88\n",
      "Episode 2140\tLast reward:   370\tAverage reward: 414.84\n",
      "Episode 2160\tLast reward:   498\tAverage reward: 420.92\n",
      "Episode 2180\tLast reward:   450\tAverage reward: 424.46\n",
      "Episode 2200\tLast reward:   466\tAverage reward: 423.50\n",
      "Episode 2220\tLast reward:   363\tAverage reward: 424.76\n",
      "Episode 2240\tLast reward:   433\tAverage reward: 421.17\n",
      "Episode 2260\tLast reward:   395\tAverage reward: 423.01\n",
      "Episode 2280\tLast reward:   429\tAverage reward: 428.27\n",
      "Episode 2300\tLast reward:   518\tAverage reward: 428.71\n",
      "Episode 2320\tLast reward:   406\tAverage reward: 424.32\n",
      "Episode 2340\tLast reward:   407\tAverage reward: 418.21\n",
      "Episode 2360\tLast reward:   184\tAverage reward: 413.12\n",
      "Episode 2380\tLast reward:   409\tAverage reward: 411.90\n",
      "Episode 2400\tLast reward:   510\tAverage reward: 416.75\n",
      "Episode 2420\tLast reward:   524\tAverage reward: 423.30\n",
      "Episode 2440\tLast reward:   398\tAverage reward: 424.18\n",
      "Episode 2460\tLast reward:   454\tAverage reward: 422.78\n",
      "Episode 2480\tLast reward:   517\tAverage reward: 427.25\n",
      "Episode 2500\tLast reward:   507\tAverage reward: 425.50\n",
      "Episode 2520\tLast reward:   405\tAverage reward: 424.47\n",
      "Episode 2540\tLast reward:   353\tAverage reward: 425.14\n",
      "Episode 2560\tLast reward:   491\tAverage reward: 429.78\n",
      "Episode 2580\tLast reward:   452\tAverage reward: 433.99\n",
      "Episode 2600\tLast reward:   306\tAverage reward: 438.00\n",
      "Episode 2620\tLast reward:   333\tAverage reward: 435.38\n",
      "Episode 2640\tLast reward:   481\tAverage reward: 437.56\n",
      "Episode 2660\tLast reward:   401\tAverage reward: 440.48\n",
      "Episode 2680\tLast reward:   366\tAverage reward: 438.60\n",
      "Episode 2700\tLast reward:   362\tAverage reward: 430.89\n",
      "Episode 2720\tLast reward:   433\tAverage reward: 425.44\n",
      "Episode 2740\tLast reward:   350\tAverage reward: 422.48\n",
      "Episode 2760\tLast reward:   384\tAverage reward: 420.55\n",
      "Episode 2780\tLast reward:   369\tAverage reward: 419.32\n",
      "Episode 2800\tLast reward:   411\tAverage reward: 414.78\n",
      "Episode 2820\tLast reward:   390\tAverage reward: 415.75\n",
      "Episode 2840\tLast reward:   445\tAverage reward: 417.75\n",
      "Episode 2860\tLast reward:   541\tAverage reward: 424.27\n",
      "Episode 2880\tLast reward:   408\tAverage reward: 421.33\n",
      "Episode 2900\tLast reward:   375\tAverage reward: 429.86\n",
      "Episode 2920\tLast reward:   440\tAverage reward: 433.39\n",
      "Episode 2940\tLast reward:   515\tAverage reward: 440.16\n",
      "Episode 2960\tLast reward:   308\tAverage reward: 441.62\n",
      "Episode 2980\tLast reward:   379\tAverage reward: 438.63\n",
      "Episode 3000\tLast reward:   296\tAverage reward: 430.06\n"
     ]
    }
   ],
   "source": [
    "# Adapted from https://github.com/pytorch/examples/blob/2dca104/reinforcement_learning/reinforce.py\n",
    "# Licensed under BSD 3-clause: https://github.com/pytorch/examples/blob/2dca10404443ce3178343c07ba6e22af13efb006/LICENSE\n",
    "\n",
    "import random\n",
    "from itertools import count\n",
    "\n",
    "from env import GatheringEnv\n",
    "# from model import Policy   Will instead use the Model in the Notebook\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "\n",
    "# We remove the argparse code and replace with hard parameters\n",
    "gamma = 0.99\n",
    "seed = 543\n",
    "render=False\n",
    "log_interval=20\n",
    "target_reward = 500\n",
    "                 \n",
    "# Start the Gathering environment\n",
    "env = GatheringEnv(n_agents=2, map_name='default')\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Load the learning agent (single)\n",
    "policy = Policy(env.state_size)\n",
    "\n",
    "# policy.load_weights()     Start training from scratch\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)   # Original lr = 1e-2\n",
    "\n",
    "# This is the task list to finish a game episode\n",
    "def finish_episode():\n",
    "    \"\"\" \n",
    "    Based on REINFORCE, policy gradient is calculated at the end of an episode.\n",
    "    It is then used to update the Policy's weights\n",
    "    \"\"\"\n",
    "    \n",
    "    # Note that the current code only accommodate 1 learning agent.\n",
    "    # Reward for each time step is stored in the list policy.rewards[] --> r(t)\n",
    "    R = 0  # noqa\n",
    "    rewards = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R  # noqa\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "    \n",
    "    # Calculate policy gradient Σ_t[r(t) ∇_θ log(π_θ(s_t,a_t))\n",
    "    for action, r in zip(policy.saved_actions, rewards):\n",
    "        action.reinforce(r)     # Note that action.reinforce has been deprecated since torch V0.3\n",
    "    optimizer.zero_grad()\n",
    "    autograd.backward(policy.saved_actions, [None for _ in policy.saved_actions])\n",
    "    optimizer.step()\n",
    "    \n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_actions[:]\n",
    "\n",
    "\n",
    "running_reward = None\n",
    "best_reward = None  # Keep track of best running average for storing best_model\n",
    "for i_episode in range(1,3001):   # For each game episode\n",
    "    \n",
    "    # Some basic initialization\n",
    "    state = env.reset()[0]\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for t in range(1000):  # Don't infinite loop while learning\n",
    "        \n",
    "        # 1. learning agent selects an action\n",
    "        action = policy.select_action(state)\n",
    "        \n",
    "        # 2. environment accepts action from learning agent and random action from 2nd agent\n",
    "        # and doles out next state, reward and if the episode is done\n",
    "        state_n, reward_n, done_n, _ = env.step([action, random.randrange(0, 8)])\n",
    "        state = state_n[0]\n",
    "        reward = reward_n[0]\n",
    "        done = done_n[0]\n",
    "        \n",
    "        if render:   # render learning if set to True\n",
    "            env.render()\n",
    "            \n",
    "        # Store rewards into array for REINFORCE    \n",
    "        policy.rewards.append(reward)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update reward statistics\n",
    "    if running_reward is None:\n",
    "        running_reward = episode_reward\n",
    "    running_reward = running_reward * 0.99 + episode_reward * 0.01\n",
    "    \n",
    "    finish_episode()    # Finish up game episode\n",
    "        \n",
    "    if i_episode % log_interval == 0:\n",
    "        print('Episode {}\\tLast reward: {:5d}\\tAverage reward: {:.2f}'.format(\n",
    "            i_episode, episode_reward, running_reward))\n",
    "        # Only save NN weights if running_reward is better than best_reward\n",
    "        if best_reward is None:\n",
    "            best_reward = running_reward\n",
    "            policy.save_weights()   # Save NN weights if this is the first save\n",
    "        else:\n",
    "            if running_reward > best_reward:\n",
    "                policy.save_weights()   # Save NN weights only if agent performs better\n",
    "        \n",
    "    if running_reward > target_reward:    # Rather weird to \n",
    "        print(\"Solved! Running reward is now {} and \"\n",
    "              \"the last episode received {} reward!\".format(running_reward, episode_reward))\n",
    "        policy.save_weights()   # Save NN weights\n",
    "        env.close()  # Close the rendering window\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand the Gathering Environment\n",
    "\n",
    "To be able to implement multiple learning agents in Gathering, we need to understand how the Class GatheringEnv() works.\n",
    "\n",
    "The environment is built on top of OpenAI Gym. The coding is a bit of a hack, and require a lot of documentation in order to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import os.path\n",
    "import tkinter as tk\n",
    "\n",
    "import gym\n",
    "import gym.envs.registration\n",
    "import gym.spaces\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# The 8 actions\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "ROTATE_RIGHT = 4\n",
    "ROTATE_LEFT = 5\n",
    "LASER = 6\n",
    "NOOP = 7\n",
    "\n",
    "\n",
    "class GatheringEnv(gym.Env):\n",
    "\n",
    "    # Some basic parameters for the Gathering Game\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    scale = 20           # Used to scale to display during rendering\n",
    "\n",
    "    # Viewbox is to implement partial observable Markov game\n",
    "    viewbox_width = 10\n",
    "    viewbox_depth = 20\n",
    "    padding = max(viewbox_width // 2, viewbox_depth - 1)  # essentially 20-1=19\n",
    "\n",
    "    # To help agents distinquish between themselves, the other agents and the apple\n",
    "    agent_colors = ['red', 'yellow']\n",
    "\n",
    "    # A function to build the game space from a text file\n",
    "    def _text_to_map(self, text):\n",
    "\n",
    "        m = [list(row) for row in text.splitlines()]  # regard \"\\r\", \"\\n\", and \"\\r\\n\" as line boundaries \n",
    "        l = len(m[0])\n",
    "        for row in m:   # Check for errors in text file\n",
    "            if len(row) != l:\n",
    "                raise ValueError('the rows in the map are not all the same length')\n",
    "\n",
    "        # This essentially add a padding of 20 cells around the region enclosed by the walls or by the\n",
    "        # food (if there is no wall). During rendering you will observe this padded region when a laser\n",
    "        # is fired     \n",
    "        def pad(a):\n",
    "            return np.pad(a, self.padding + 1, 'constant')\n",
    "\n",
    "        a = np.array(m).T\n",
    "        self.initial_food = pad(a == 'O').astype(np.int)    # Pad 20 around food\n",
    "        self.walls = pad(a == '#').astype(np.int)           # Pad 20 around the walls\n",
    "\n",
    "\n",
    "    # This is run when the environment is created\n",
    "    def __init__(self, n_agents=1, map_name='default'):\n",
    "\n",
    "        self.n_agents = n_agents    # Set number of agents\n",
    "        self.root = None            # For rendering\n",
    "\n",
    "        # Create game space from text file\n",
    "        if not os.path.exists(map_name):\n",
    "            expanded = os.path.join('maps', map_name + '.txt')\n",
    "            if not os.path.exists(expanded):\n",
    "                raise ValueError('map not found: ' + map_name)\n",
    "            map_name = expanded\n",
    "        with open(map_name) as f:\n",
    "            self._text_to_map(f.read().strip())    # This sets up self.initial_food and self.walls\n",
    "\n",
    "        # Populate the rest of environment parameters\n",
    "        self.width = self.initial_food.shape[0]\n",
    "        self.height = self.initial_food.shape[1]\n",
    "\n",
    "        # This is a partial observable Markov game. So the state for each agent is a single frame of the  \n",
    "        # 10x20 of RGB pixels (view box). There is no stacked frames.\n",
    "        self.state_size = self.viewbox_width * self.viewbox_depth * 4\n",
    "        self.observation_space = gym.spaces.MultiDiscrete([[[0, 1]] * self.state_size] * n_agents)\n",
    "        self.action_space = gym.spaces.MultiDiscrete([[0, 7]] * n_agents)   # Action space for n agents\n",
    "\n",
    "        self._spec = gym.envs.registration.EnvSpec(**_spec)\n",
    "        self.reset()    # Reset environment\n",
    "        self.done = False\n",
    "\n",
    "    # A function to take the game one step forward\n",
    "    # Inputs: a list of actions indexed by agent\n",
    "    def _step(self, action_n):\n",
    "\n",
    "        assert len(action_n) == self.n_agents  # Error check for action list\n",
    "        # Set action of tagged agents to NOOP\n",
    "        action_n = [NOOP if self.tagged[i] else a for i, a in enumerate(action_n)]\n",
    "\n",
    "        # Initialize variables for movement and for beam\n",
    "        self.beams[:] = 0\n",
    "        movement_n = [(0, 0) for a in action_n]\n",
    "\n",
    "        # Update movement if action is UP, DOWN, RIGHT or LEFT\n",
    "        for i, (a, orientation) in enumerate(zip(action_n, self.orientations)):\n",
    "            if a not in [UP, DOWN, LEFT, RIGHT]:\n",
    "                continue\n",
    "            # a is relative to the agent's orientation, so add the orientation\n",
    "            # before interpreting in the global coordinate system.\n",
    "            #\n",
    "            # This line is really not obvious to read. Replace it with something\n",
    "            # clearer if you have a better idea.\n",
    "            a = (a + orientation) % 4\n",
    "            movement_n[i] = [\n",
    "                (0, -1),  # up/forward\n",
    "                (1, 0),   # right\n",
    "                (0, 1),   # down/backward\n",
    "                (-1, 0),  # left\n",
    "            ][a]\n",
    "\n",
    "        # The code section below updates agent location based on actions that are movements    \n",
    "        next_locations = [a for a in self.agents]  # Initialize next_locations\n",
    "        # If a key is not found in the dictionary, then instead of a KeyError being thrown, a new entry \n",
    "        # is created.\n",
    "        next_locations_map = collections.defaultdict(list)\n",
    "\n",
    "\n",
    "        for i, ((dx, dy), (x, y)) in enumerate(zip(movement_n, self.agents)):  # For each agent\n",
    "            if self.tagged[i]:\n",
    "                continue        # skip agents that are tagged\n",
    "            next_ = ((x + dx), (y + dy))   # Update next location\n",
    "            if self.walls[next_]:\n",
    "                next_ = (x, y)              # Do not move beyond wall\n",
    "            next_locations[i] = next_\n",
    "            next_locations_map[next_].append(i)   # Add agent number to next_location_map (???)\n",
    "\n",
    "        # If there are more than 1 agent in the same location\n",
    "        for overlappers in next_locations_map.values():\n",
    "            if len(overlappers) > 1:\n",
    "                for i in overlappers:\n",
    "                    next_locations[i] = self.agents[i]  # return agent to their previous location\n",
    "        self.agents = next_locations_map    # Update agent locations\n",
    "\n",
    "        # If action is ROTATE_RIGHT, ROTATE_LEFT or LASER\n",
    "        for i, act in enumerate(action_n):\n",
    "            if act == ROTATE_RIGHT:\n",
    "                self.orientations[i] = (self.orientations[i] + 1) % 4\n",
    "            elif act == ROTATE_LEFT:\n",
    "                self.orientations[i] = (self.orientations[i] - 1) % 4\n",
    "            elif act == LASER:\n",
    "                # Laser beam is 5x20 along the orientation of the agent\n",
    "                self.beams[self._viewbox_slice(i, 5, 20, offset=1)] = 1   \n",
    "\n",
    "        # Prepare obs_n, reward_n, done_n and info_n to be returned        \n",
    "        obs_n = self.state_n    # obs_n is self.state_n\n",
    "        reward_n = [0 for _ in range(self.n_agents)]\n",
    "        done_n = [self.done] * self.n_agents\n",
    "        info_n = [{}] * self.n_agents\n",
    "\n",
    "        # This is really shitty code writing. If agent lands on a food cell, that cell is set to -15.\n",
    "        # Then for each subsequent step, it is incremented by 1 until it reaches 1 again.\n",
    "        # self.initial_food is the game space created from the text file whereby the cell with food \n",
    "        # is given the value of 1, every other cell has the value of 0.\n",
    "        self.food = (self.food + self.initial_food).clip(max=1)\n",
    "\n",
    "        # In case the agent lands on a cell with food, or is tagged\n",
    "        for i, a in enumerate(self.agents):\n",
    "            if self.tagged[i]:\n",
    "                continue\n",
    "            if self.food[a] == 1:\n",
    "                self.food[a] = -15    # Food is respawned every 15 steps once it has been consumed\n",
    "                reward_n[i] = 1       # Agent is given reward of 1\n",
    "            if self.beams[a]:\n",
    "                self.tagged[i] = 25   # If agent is tagged, it is removed from the game for 25 steps\n",
    "\n",
    "        # Respawn agent after 25 steps\n",
    "        for i, tag in enumerate(self.tagged):\n",
    "            if tag == 1:\n",
    "                # Relocate to a respawn point.\n",
    "                for spawn_point in self.spawn_points:\n",
    "                    if spawn_point not in self.agents:\n",
    "                        self.agents[i] = spawn_point\n",
    "                        break\n",
    "\n",
    "        # This is where the tagged counter (25 steps) is updated\n",
    "        self.tagged = [max(i - 1, 0) for i in self.tagged]   \n",
    "\n",
    "        return obs_n, reward_n, done_n, info_n\n",
    "\n",
    "\n",
    "    # Generate slice(tuple) to slice out observation space for agents\n",
    "    def _viewbox_slice(self, agent_index, width, depth, offset=0):\n",
    "        \n",
    "        # These are inputs for generating an observation space for the agent\n",
    "        left = width // 2\n",
    "        right = left if width % 2 == 0 else left + 1\n",
    "        x, y = self.agents[agent_index]\n",
    "\n",
    "        # This is really hard-to-read code. Essentially, it generates the observation\n",
    "        # spaces for an agent in all 4 orientations, then only return the one indexed\n",
    "        # by its current orientation.\n",
    "        # Note: itertools.starmap maps the orientation-indexed tuple to slice()\n",
    "        return tuple(itertools.starmap(slice, (\n",
    "            ((x - left, x + right), (y - offset, y - offset - depth, -1)),      # up\n",
    "            ((x + offset, x + offset + depth), (y - left, y + right)),          # right\n",
    "            ((x + left, x - right, -1), (y + offset, y + offset + depth)),      # down\n",
    "            ((x - offset, x - offset - depth, -1), (y + left, y - right, -1)),  # left\n",
    "        )[self.orientations[agent_index]]))\n",
    "\n",
    "\n",
    "    # state_n (next state) is a property object. So this function is run everytime state_n is\n",
    "    # called as a varaiable.\n",
    "    @property\n",
    "    def state_n(self):\n",
    "\n",
    "        # Create a game space for agent locating\n",
    "        agents = np.zeros_like(self.food)  \n",
    "\n",
    "        # self.agent is a list of agent locations indexed by agent index\n",
    "        for i, loc in enumerate(self.agents):    \n",
    "            if not self.tagged[i]:\n",
    "                agents[loc] = 1     # Mark the agent's location\n",
    "\n",
    "        food = self.food.clip(min=0)   # Mark the food's location\n",
    "\n",
    "        # Zero out next states for the agents\n",
    "        s = np.zeros((self.n_agents, self.viewbox_width, self.viewbox_depth, 4))\n",
    "\n",
    "        # Enumerate index, (agent orientation, agent location) by agent index\n",
    "        for i, (orientation, (x, y)) in enumerate(zip(self.orientations, self.agents)):\n",
    "\n",
    "            if self.tagged[i]:\n",
    "                continue     # Skip if agent has been tagged out of the game\n",
    "\n",
    "            # If agent is not tagged, ....\n",
    "\n",
    "            # Construct the full state for the game, which consists of:\n",
    "            # 1. Location of Food\n",
    "            # 2. ???\n",
    "            # 3. Location of agents\n",
    "            # 4. Location of the walls\n",
    "            full_state = np.stack([food, np.zeros_like(food), agents, self.walls], axis=-1)\n",
    "            full_state[x, y, 2] = 0   # Zero out the agent's location ???\n",
    "\n",
    "            # Create observation space for learning agent using _viewbox_slice()\n",
    "            xs, ys = self._viewbox_slice(i, self.viewbox_width, self.viewbox_depth)\n",
    "            observation = full_state[xs, ys, :]\n",
    "\n",
    "            # Orient the observation space correctly\n",
    "            s[i] = observation if orientation in [UP, DOWN] else observation.transpose(1, 0, 2)\n",
    "\n",
    "        return s.reshape((self.n_agents, self.state_size))  # Return the agents' observations\n",
    "\n",
    "\n",
    "    # To reset the environment\n",
    "    def _reset(self):\n",
    "\n",
    "        # Build food stash\n",
    "        self.food = self.initial_food.copy()\n",
    "\n",
    "\n",
    "        # Rebuild the wall (by subtracting padding from self.walls - very weird implementation!!!)\n",
    "        # Essentially, think of a much larger game area (+20 cells on each side) surrounding the \n",
    "        # walled region.\n",
    "        p = self.padding\n",
    "        self.walls[p:-p, p] = 1\n",
    "        self.walls[p:-p, -p - 1] = 1\n",
    "        self.walls[p, p:-p] = 1\n",
    "        self.walls[-p - 1, p:-p] = 1\n",
    "\n",
    "        self.beams = np.zeros_like(self.food)  # self.beams region is as big as self.food (weird!)\n",
    "\n",
    "        # Set up agent parameters\n",
    "        # The agents are spawned at the right upper corner of the game area, one next to the other\n",
    "        self.agents = [(i + self.padding + 1, self.padding + 1) for i in range(self.n_agents)]\n",
    "        self.spawn_points = list(self.agents)\n",
    "        self.orientations = [UP for _ in self.agents]   # Orientation = UP\n",
    "        self.tagged = [0 for _ in self.agents]          # Tagged = False\n",
    "\n",
    "        return self.state_n  # Since state_n is a property object, so it will call function _state_n()\n",
    "\n",
    "\n",
    "    # To close the rendering window\n",
    "    def _close_view(self):\n",
    "        # If rendering window is active, close it\n",
    "        if self.root:\n",
    "            self.root.destroy()\n",
    "            self.root = None\n",
    "            self.canvas = None\n",
    "        self.done = True   # The episode is done\n",
    "    \n",
    "\n",
    "    # TO render the game    \n",
    "    def _render(self, mode='human', close=False):\n",
    "        if close:\n",
    "            self._close_view()\n",
    "            return\n",
    "\n",
    "        canvas_width = self.width * self.scale\n",
    "        canvas_height = self.height * self.scale\n",
    "\n",
    "        if self.root is None:\n",
    "            self.root = tk.Tk()\n",
    "            self.root.title('Gathering')\n",
    "            self.root.protocol('WM_DELETE_WINDOW', self._close_view)\n",
    "            self.canvas = tk.Canvas(self.root, width=canvas_width, height=canvas_height)\n",
    "            self.canvas.pack()\n",
    "\n",
    "        self.canvas.delete(tk.ALL)\n",
    "        self.canvas.create_rectangle(0, 0, canvas_width, canvas_height, fill='black')\n",
    "\n",
    "        def fill_cell(x, y, color):\n",
    "            self.canvas.create_rectangle(\n",
    "                x * self.scale,\n",
    "                y * self.scale,\n",
    "                (x + 1) * self.scale,\n",
    "                (y + 1) * self.scale,\n",
    "                fill=color,\n",
    "            )\n",
    "\n",
    "        for x in range(self.width):\n",
    "            for y in range(self.height):\n",
    "                if self.beams[x, y] == 1:\n",
    "                    fill_cell(x, y, 'yellow')\n",
    "                if self.food[x, y] == 1:\n",
    "                    fill_cell(x, y, 'green')\n",
    "                if self.walls[x, y] == 1:\n",
    "                    fill_cell(x, y, 'grey')\n",
    "\n",
    "        for i, (x, y) in enumerate(self.agents):\n",
    "            if not self.tagged[i]:\n",
    "                fill_cell(x, y, self.agent_colors[i])\n",
    "\n",
    "        if False:\n",
    "            # Debug view: see the first player's viewbox perspective.\n",
    "            p1_state = self.state_n[0].reshape(self.viewbox_width, self.viewbox_depth, 4)\n",
    "            for x in range(self.viewbox_width):\n",
    "                for y in range(self.viewbox_depth):\n",
    "                    food, me, other, wall = p1_state[x, y]\n",
    "                    assert sum((food, me, other, wall)) <= 1\n",
    "                    y_ = self.viewbox_depth - y - 1\n",
    "                    if food:\n",
    "                        fill_cell(x, y_, 'green')\n",
    "                    elif me:\n",
    "                        fill_cell(x, y_, 'cyan')\n",
    "                    elif other:\n",
    "                        fill_cell(x, y_, 'red')\n",
    "                    elif wall:\n",
    "                        fill_cell(x, y_, 'gray')\n",
    "            self.canvas.create_rectangle(\n",
    "                0,\n",
    "                0,\n",
    "                self.viewbox_width * self.scale,\n",
    "                self.viewbox_depth * self.scale,\n",
    "                outline='blue',\n",
    "            )\n",
    "\n",
    "        self.root.update()\n",
    "\n",
    "\n",
    "    # To close the environment\n",
    "    def _close(self):\n",
    "        self._close_view()\n",
    "\n",
    "    # To delete the environment\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "\n",
    "_spec = {\n",
    "    'id': 'Gathering-Luke-v001',\n",
    "    'entry_point': GatheringEnv,\n",
    "    'reward_threshold': 500,   # The environment threshold at 100 appears to be too low\n",
    "}\n",
    "\n",
    "\n",
    "gym.envs.registration.register(**_spec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figuring out data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 47)\n",
      "[(20, 20), (21, 20)]\n",
      "[0, 0]\n",
      "0 (20, 20)\n",
      "1 (21, 20)\n",
      "0 (0, (20, 20))\n",
      "1 (0, (21, 20))\n"
     ]
    }
   ],
   "source": [
    "# Start the Gathering environment\n",
    "env = GatheringEnv(n_agents=2, map_name='small')\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "print (env.food.shape)\n",
    "print (env.agents)\n",
    "print (env.orientations)\n",
    "for i,a in enumerate(env.agents):\n",
    "    print (i, a)\n",
    "    \n",
    "for i, (orientation, (x, y)) in enumerate(zip(env.orientations, env.agents)):\n",
    "    print (i, (orientation, (x, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure out array manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[[0, 1]] * 3] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 2 3 4]\n",
      "  [1 2 3 4]\n",
      "  [1 2 3 4]]]\n",
      "[[[1 2 3 4]\n",
      "  [0 0 0 0]\n",
      "  [1 2 3 4]]]\n",
      "[[[1 2 0 4]\n",
      "  [0 0 0 0]\n",
      "  [1 2 0 4]]]\n"
     ]
    }
   ],
   "source": [
    "a = [[1,1,1]]\n",
    "b = [[2,2,2]]\n",
    "c = [[3,3,3]]\n",
    "d = [[4,4,4]]\n",
    "\n",
    "full_state = np.stack([a,b,c,d], axis=-1)\n",
    "print (full_state)\n",
    "\n",
    "full_state[:,1,:]=0\n",
    "print (full_state)\n",
    "\n",
    "full_state[:,:,2]=0\n",
    "print (full_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figuring out _view_slice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(slice(10, 20, None), slice(20, 5, -1))\n",
      "(slice(15, 30, None), slice(15, 25, None))\n",
      "(slice(20, 10, -1), slice(20, 35, None))\n",
      "(slice(15, 0, -1), slice(25, 15, -1))\n"
     ]
    }
   ],
   "source": [
    "width = 10\n",
    "depth = 15\n",
    "offset = 0\n",
    "x = 15\n",
    "y = 20\n",
    "\n",
    "# These are inputs for generating an observation space for the agent\n",
    "left = width // 2\n",
    "right = left if width % 2 == 0 else left + 1\n",
    "\n",
    "for orient in range(4):\n",
    "    viewbox = tuple(itertools.starmap(slice, (\n",
    "            ((x - left, x + right), (y - offset, y - offset - depth, -1)),      # up\n",
    "            ((x + offset, x + offset + depth), (y - left, y + right)),          # right\n",
    "            ((x + left, x - right, -1), (y + offset, y + offset + depth)),      # down\n",
    "            ((x - offset, x - offset - depth, -1), (y + left, y - right, -1)),\n",
    "        )[orient]))\n",
    "\n",
    "    print (viewbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
