{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Team Gathering Environment\n",
    "\n",
    "The Gathering Game is a miniworld composed of:\n",
    "* Agents (Gatherers)\n",
    "* Resource (Food units) - a source of positive rewards provided by the environment\n",
    "* Weaponry (Laser) - the ability to inflict negative reward built into the environment\n",
    "\n",
    "The way we organize the rewards of the agents can transform the game and the problem:\n",
    "* **A world of selfish lone beings** - It is equivalent to a world with a horse, a sheep, a hog, etc. There can be neither context nor reason for them to cooperate for the common good. Cooperation has to come about due to selfishness and reciprocity.\n",
    "* **A world of communal beings** - It is equivalent to a world of psuedo-communists. All rewards gathered are shared by default. Cooperation comes about due to the need to maximize total reward.\n",
    "* **A world of competing tribes** - It is equivalent to a world of 2 tribes of psuedo-communists. All rewards gathered by a tribe are shared within by tribe members. Cooperation within tribe comes about due to the need to maximize total reward for the tribe. Cooperation between tribes still has to come about due to selfishness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.6.4\n",
      "Pytorch version: 0.4.1.post2\n",
      "OpenAI Gym version: 0.9.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import platform\n",
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from env import GatheringEnv   # This is the Game Environment\n",
    "from model import *   # Use the Policy defined below instead\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Python version: \", platform.python_version())\n",
    "print(\"Pytorch version: {}\".format(torch.__version__))\n",
    "print(\"OpenAI Gym version: {}\".format(gym.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load agent 0\n",
      "Load agent 1\n",
      "Load agent 2\n",
      "Load agent 3\n",
      "[tensor(4), tensor(4), tensor(2), 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukeai/Documents/ssd/model.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(action_scores)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1), tensor(4), tensor(6), 4]\n",
      "[tensor(0), tensor(4), tensor(1), 1]\n",
      "[tensor(6), tensor(1), tensor(6), 2]\n",
      "[tensor(0), tensor(1), tensor(6), 2]\n",
      "[tensor(0), tensor(5), tensor(6), 6]\n",
      "[tensor(1), tensor(4), tensor(5), 4]\n",
      "[tensor(4), tensor(1), tensor(6), 6]\n",
      "[tensor(4), tensor(1), tensor(6), 4]\n",
      "[tensor(6), tensor(1), tensor(0), 2]\n",
      "[tensor(0), tensor(0), tensor(2), 6]\n",
      "[tensor(6), tensor(4), tensor(6), 7]\n",
      "[tensor(4), tensor(4), tensor(2), 7]\n",
      "[tensor(1), tensor(7), tensor(0), 7]\n",
      "[tensor(6), tensor(4), tensor(0), 7]\n",
      "[tensor(0), tensor(0), tensor(0), 7]\n",
      "[tensor(4), tensor(3), tensor(0), 0]\n",
      "[tensor(6), tensor(6), tensor(1), 4]\n",
      "[tensor(1), tensor(2), tensor(0), 4]\n",
      "[tensor(5), tensor(2), tensor(6), 5]\n",
      "[tensor(4), tensor(4), tensor(0), 0]\n",
      "[tensor(0), tensor(1), tensor(0), 1]\n",
      "[tensor(5), tensor(5), tensor(1), 5]\n",
      "[tensor(4), tensor(1), tensor(6), 2]\n",
      "[tensor(6), tensor(0), tensor(0), 0]\n",
      "[tensor(4), tensor(0), tensor(0), 5]\n",
      "[tensor(4), tensor(6), tensor(0), 6]\n",
      "[tensor(3), tensor(0), tensor(6), 4]\n",
      "[tensor(4), tensor(0), tensor(0), 5]\n",
      "[tensor(6), tensor(6), tensor(7), 6]\n",
      "[tensor(5), tensor(5), tensor(0), 3]\n",
      "[tensor(4), tensor(6), tensor(5), 0]\n",
      "[tensor(4), tensor(5), tensor(6), 4]\n",
      "[tensor(1), tensor(1), tensor(6), 4]\n",
      "[tensor(2), tensor(6), tensor(0), 3]\n",
      "[tensor(2), tensor(0), tensor(0), 6]\n",
      "[tensor(0), tensor(6), tensor(0), 0]\n",
      "[tensor(1), tensor(5), tensor(0), 5]\n",
      "[tensor(7), tensor(0), tensor(7), 3]\n",
      "[tensor(4), tensor(6), tensor(1), 4]\n",
      "[tensor(4), tensor(1), tensor(0), 0]\n",
      "[tensor(4), tensor(1), tensor(2), 4]\n",
      "[tensor(4), tensor(0), tensor(0), 0]\n",
      "[tensor(0), tensor(3), tensor(1), 0]\n",
      "[tensor(1), tensor(4), tensor(1), 7]\n",
      "[tensor(0), tensor(2), tensor(0), 2]\n",
      "[tensor(4), tensor(0), tensor(3), 3]\n",
      "[tensor(1), tensor(0), tensor(6), 2]\n",
      "[tensor(6), tensor(0), tensor(0), 7]\n",
      "[tensor(1), tensor(6), tensor(4), 7]\n",
      "[tensor(5), tensor(1), tensor(6), 3]\n",
      "[tensor(4), tensor(5), tensor(2), 0]\n",
      "[tensor(4), tensor(5), tensor(7), 3]\n",
      "[tensor(1), tensor(6), tensor(2), 4]\n",
      "[tensor(7), tensor(6), tensor(5), 7]\n",
      "[tensor(2), tensor(5), tensor(5), 3]\n",
      "[tensor(2), tensor(0), tensor(0), 5]\n",
      "[tensor(6), tensor(6), tensor(0), 2]\n",
      "[tensor(6), tensor(6), tensor(5), 1]\n",
      "[tensor(2), tensor(6), tensor(6), 0]\n",
      "[tensor(6), tensor(1), tensor(0), 4]\n",
      "[tensor(4), tensor(5), tensor(0), 1]\n",
      "[tensor(4), tensor(6), tensor(6), 1]\n",
      "[tensor(0), tensor(0), tensor(0), 0]\n",
      "[tensor(5), tensor(1), tensor(6), 3]\n",
      "[tensor(1), tensor(0), tensor(4), 6]\n",
      "[tensor(0), tensor(6), tensor(0), 3]\n",
      "[tensor(1), tensor(6), tensor(1), 3]\n",
      "[tensor(0), tensor(1), tensor(0), 7]\n",
      "[tensor(0), tensor(1), tensor(1), 7]\n",
      "[tensor(0), tensor(0), tensor(0), 3]\n",
      "[tensor(3), tensor(5), tensor(6), 5]\n",
      "[tensor(6), tensor(6), tensor(0), 2]\n",
      "[tensor(1), tensor(0), tensor(4), 2]\n",
      "[tensor(4), tensor(0), tensor(0), 5]\n",
      "[tensor(6), tensor(6), tensor(0), 2]\n",
      "[tensor(6), tensor(6), tensor(1), 5]\n",
      "[tensor(6), tensor(5), tensor(1), 1]\n",
      "[tensor(7), tensor(5), tensor(0), 6]\n",
      "[tensor(4), tensor(6), tensor(6), 5]\n",
      "[tensor(1), tensor(6), tensor(1), 2]\n",
      "[tensor(7), tensor(0), tensor(1), 4]\n",
      "[tensor(4), tensor(0), tensor(0), 4]\n",
      "[tensor(6), tensor(0), tensor(0), 2]\n",
      "[tensor(1), tensor(6), tensor(6), 2]\n",
      "[tensor(4), tensor(1), tensor(4), 6]\n",
      "[tensor(6), tensor(6), tensor(1), 3]\n",
      "[tensor(6), tensor(6), tensor(5), 5]\n",
      "[tensor(1), tensor(5), tensor(0), 2]\n",
      "[tensor(2), tensor(6), tensor(0), 6]\n",
      "[tensor(1), tensor(0), tensor(5), 4]\n",
      "[tensor(6), tensor(5), tensor(0), 2]\n",
      "[tensor(2), tensor(0), tensor(1), 1]\n",
      "[tensor(0), tensor(5), tensor(3), 2]\n",
      "[tensor(0), tensor(4), tensor(3), 6]\n",
      "[tensor(1), tensor(1), tensor(0), 4]\n",
      "[tensor(5), tensor(3), tensor(3), 1]\n",
      "[tensor(5), tensor(0), tensor(6), 1]\n",
      "[tensor(1), tensor(1), tensor(0), 6]\n",
      "[tensor(3), tensor(3), tensor(0), 6]\n",
      "Agent1 aggressiveness is 0.00\n",
      "Agent1 reward is 69\n",
      "Agent2 aggressiveness is 0.00\n",
      "Agent2 reward is 220\n",
      "Agent3 aggressiveness is 0.00\n",
      "Agent3 reward is 2\n",
      "Agent4 aggressiveness is 0.13\n",
      "Agent4 reward is 0\n"
     ]
    }
   ],
   "source": [
    "from model import *    # Use the Policy and Rdn_policy defined in model.py\n",
    "\n",
    "# There will be 4 agents - 3 AI agents, 1 random agent\n",
    "num_ai_agents = 3\n",
    "num_rdn_agents = 1\n",
    "num_agents = num_ai_agents+num_rdn_agents  # just the sum of the two\n",
    "\n",
    "# Data structure for AI agents (agents will form their own Class later on)\n",
    "ai_agents = []\n",
    "actions = []\n",
    "tags = []\n",
    "rewards = []\n",
    "\n",
    "env = GatheringEnv(n_agents=num_agents, map_name='default')\n",
    "\n",
    "# Env API is similar to that of OpenAI Gym\n",
    "state_n = env.reset()\n",
    "env.render()\n",
    "\n",
    "# Load AI agents with trained weights\n",
    "for i in range(num_ai_agents):\n",
    "    print(\"Load agent {}\".format(i))\n",
    "    ai_agents.append(Policy(env.state_size, i+1))\n",
    "    ai_agents[i].load_weights()\n",
    "# Load random agents    \n",
    "for i in range(num_ai_agents,num_agents):\n",
    "    print(\"Load agent {}\".format(i))\n",
    "    ai_agents.append(Rdn_Policy())\n",
    "\n",
    "# Initialize AI and random agent data\n",
    "for i in range(num_agents):\n",
    "    actions = [0 for i in range(num_agents)]\n",
    "    tags = [0 for i in range(num_agents)]\n",
    "    rewards = [0 for i in range(num_agents)]\n",
    "\n",
    "n_steps = 1000\n",
    "\n",
    "# Render for n_steps steps\n",
    "for step in range(n_steps):\n",
    "    # Load AI agent with trained weights\n",
    "    for i in range(num_agents):\n",
    "        actions[i] = ai_agents[i].select_action(state_n[i])\n",
    "        if actions[i] is 6:\n",
    "            tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print (actions)    \n",
    "            \n",
    "    state_n, reward_n, done_n, info_n = env.step(actions)\n",
    "\n",
    "    for i in range(num_agents):\n",
    "        rewards[i] += reward_n[i]    # Accumulate rewards for each agent\n",
    "        \n",
    "    if any(done_n):\n",
    "        break\n",
    "    env.render()\n",
    "    time.sleep(1/30)  # Change speed of video rendering\n",
    "\n",
    "env.close()  # Close the rendering window\n",
    "\n",
    "# Print out statistics of all agents\n",
    "for i in range(num_agents):\n",
    "    print (\"Agent{} aggressiveness is {:.2f}\".format(i+1, tags[i]/n_steps))\n",
    "    print (\"Agent{} reward is {:d}\".format(i+1, rewards[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[[0, 1]] * 10] * 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = gym.spaces.MultiDiscrete([4,1,1 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray([4,1,1], dtype=np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
